A Transformer-based architecture, specifically BERT (bert-base-multilingual-cased), is employed using TensorFlow. The model is trained on question pairs for a similarity classification task. Positive samples consist of similar question pairs, while negative samples involve random question pairs. The pretrained multilingual BERT model from HuggingFace is utilized to obtain embeddings for each input.Eventually, as a result the model generates similarity scores for question pairs in the test set.